{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project Team2\n",
    "\n",
    "Members:\n",
    "- Martim Teixeira\n",
    "- Parisa Eimanzadeh\n",
    "- YuHsuan Wu\n",
    "- Anupama Bhat\n",
    "- Sai Sreekar Nelakonda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (3.1.1)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.15.1)\n",
      "Requirement already satisfied: cryptography<42.0.0,>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.1)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.3.0)\n",
      "Requirement already satisfied: pyOpenSSL<24.0.0,>=16.2.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.18.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.7.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.5.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.6.3)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.12.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<3.9.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.8.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.12.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Snowflake connector library\n",
    "import snowflake.connector\n",
    "\n",
    "# Establish a connection to the Snowflake database\n",
    "# The `connect` method takes several parameters to authenticate and specify the Snowflake account to connect to\n",
    "conn = snowflake.connector.connect(\n",
    "    user='MATEIXEIRA',\n",
    "    password='',\n",
    "    account='TIB91679'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new cursor object using the `cursor` method from the established Snowflake connection\n",
    "# The cursor object allows you to execute SQL queries and fetch results\n",
    "cs = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fb3cbb90890>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A SQL command to create a new Snowflake warehouse named \"Project_Team2\"\n",
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS Project_Team2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fb3cbb90890>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A SQL command to create a new Snowflake database named \"Project_Team2\"\n",
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS Project_Team2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fb3cbb90890>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to the \"PROJECT_TEAM2\" database\n",
    "cs.execute(\"USE PROJECT_TEAM2;\")\n",
    "\n",
    "# Create a new schema named \"Work\" if it doesn't already exist\n",
    "cs.execute(\"CREATE SCHEMA IF NOT EXISTS Work\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-1.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-10.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-11.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-12.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-2.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-3.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-4.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-5.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-6.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-7.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-8.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2013-9.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-1.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-10.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-11.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-12.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-2.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-3.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-4.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-5.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-6.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-7.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-8.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2014-9.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-1.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-10.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-11.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-12.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-2.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-3.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-4.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-5.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-6.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-7.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-8.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2015-9.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-1.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-2.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-3.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-4.csv\n",
      "Uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-5.csv\n",
      "Completed uploading ./Case_Data/Monthly_PO_Data/Monthly_PO_Data/2016-5.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd3a8170c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Switch to the \"PROJECT_TEAM2.WORK\" schema\n",
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "# Create or replace the \"purchases\" table with specified columns and data types\n",
    "cs.execute(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TABLE purchases (\n",
    "        PurchaseOrderID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        OrderDate DATE,\n",
    "        DeliveryMethodID INTEGER,\n",
    "        ContactPersonID INTEGER,\n",
    "        ExpectedDeliveryDate DATE,\n",
    "        SupplierReference STRING,\n",
    "        IsOrderFinalized BOOLEAN,\n",
    "        Comments STRING,\n",
    "        InternalComments STRING,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen STRING,\n",
    "        PurchaseOrderLineID INTEGER,\n",
    "        StockItemID INTEGER,\n",
    "        OrderedOuters INTEGER,\n",
    "        Description STRING,\n",
    "        ReceivedOuters INTEGER,\n",
    "        PackageTypeID FLOAT,\n",
    "        ExpectedUnitPricePerOuter FLOAT,\n",
    "        LastReceiptDate STRING,\n",
    "        IsOrderLineFinalized BOOLEAN,\n",
    "        Right_LastEditedBy INTEGER,\n",
    "        Right_LastEditedWhen STRING\n",
    ");\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Switch to the \"PROJECT_TEAM2.WORK\" schema again (redundant but safe)\n",
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "# Create or replace a stage named \"purchase_stg\" with CSV file format\n",
    "cs.execute(\"CREATE OR REPLACE STAGE purchase_stg FILE_FORMAT = (TYPE = 'CSV')\")\n",
    "\n",
    "# Import the glob library to find all CSV files in a directory\n",
    "import glob\n",
    "\n",
    "# Get a list of all CSV files in the specified directory\n",
    "csv_files = glob.glob(\"./Case_Data/Monthly_PO_Data/Monthly_PO_Data/*.csv\")\n",
    "\n",
    "# Define the Snowflake stage to upload files to\n",
    "stage = 'PROJECT_TEAM2.WORK.PURCHASE_STG'\n",
    "\n",
    "# Loop through each CSV file to upload it to the Snowflake stage\n",
    "for file_path in csv_files:\n",
    "    print(f\"Uploading {file_path}\")\n",
    "\n",
    "    # Upload file to the Snowflake stage\n",
    "    cs.execute(f\"PUT file://{file_path} @{stage}\")\n",
    "\n",
    "# Copy the data from the stage to the \"purchases\" table\n",
    "cs.execute(f\"COPY INTO PURCHASES FROM @{stage} FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER = 1) ON_ERROR = 'CONTINUE'\")\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Completed uploading {file_path}\")\n",
    "\n",
    "\n",
    "# Drop the \"COMMENTS\" and \"INTERNALCOMMENTS\" columns from the \"purchases\" table\n",
    "cs.execute(\"ALTER TABLE PURCHASES DROP COLUMN COMMENTS,INTERNALCOMMENTS;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fb3cbb90890>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"SELECT * FROM PURCHASES LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2025, 4, datetime.date(2016, 5, 2), 7, 2, datetime.date(2016, 5, 22), '293092', True, 'NULL', 'NULL', 10, datetime.datetime(2016, 5, 3, 7, 0), 8158, 77, 3265, '\"The Gu\" red shirt XML tag t-shirt (White) XXS', 3265, 6, Decimal('84.00'), datetime.date(2016, 5, 3), True, 10, datetime.datetime(2016, 5, 3, 7, 0)), (2025, 4, datetime.date(2016, 5, 2), 7, 2, datetime.date(2016, 5, 22), '293092', True, 'NULL', 'NULL', 10, datetime.datetime(2016, 5, 3, 7, 0), 8159, 78, 3599, '\"The Gu\" red shirt XML tag t-shirt (White) XS', 3599, 6, Decimal('84.00'), datetime.date(2016, 5, 3), True, 10, datetime.datetime(2016, 5, 3, 7, 0))]\n"
     ]
    }
   ],
   "source": [
    "print(cs.fetchmany(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(204, 4, datetime.date(2013, 5, 1), 7, 2, datetime.date(2013, 5, 21), '293092', True, 'NULL', 'NULL', 10, datetime.datetime(2013, 5, 2, 7, 0), 832, 77, 263, '\"\"\"The Gu\"\" red shirt XML tag t-shirt (White) XXS\"', 263, 6, Decimal('84.00'), datetime.date(2013, 5, 2), True, 10, datetime.datetime(2013, 5, 2, 7, 0)), (204, 4, datetime.date(2013, 5, 1), 7, 2, datetime.date(2013, 5, 21), '293092', True, 'NULL', 'NULL', 10, datetime.datetime(2013, 5, 2, 7, 0), 833, 78, 322, '\"\"\"The Gu\"\" red shirt XML tag t-shirt (White) XS\"', 322, 6, Decimal('84.00'), datetime.date(2013, 5, 2), True, 10, datetime.datetime(2013, 5, 2, 7, 0))]\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"SELECT * FROM PURCHASES LIMIT 10\")\n",
    "print(cs.fetchmany(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd43420d5d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column named \"POAmount\" with data type INT to the \"purchases\" table\n",
    "cs.execute(\"ALTER TABLE PURCHASES ADD COLUMN POAmount INT;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd43420d5d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the \"POAmount\" column in the \"purchases\" table with the sum of (ReceivedOuters * ExpectedUnitPricePerOuter)\n",
    "# for each unique \"PurchaseOrderId\"\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "UPDATE PURCHASES p1\n",
    "SET POAmount = (\n",
    "    SELECT SUM(p2.ReceivedOuters * p2.ExpectedUnitPricePerOuter)\n",
    "    FROM PURCHASES p2\n",
    "    WHERE p2.PurchaseOrderId = p1.PurchaseOrderId\n",
    ")\n",
    "WHERE EXISTS (\n",
    "    SELECT 1\n",
    "    FROM PURCHASES p2\n",
    "    WHERE p2.PurchaseOrderId = p1.PurchaseOrderId\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd43420d5d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "# Create a new stage for supplier invoices\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS invoice_stg\")\n",
    "\n",
    "# Upload the supplier invoice XML file to the supplier invoices stage\n",
    "cs.execute(\"\"\"\n",
    "    PUT 'file://./Case_Data/Supplier_Transactions_XML.xml' \n",
    "@invoice_stg auto_compress=false\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Create a table with one column of type variant\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE supplier_invoices_var (\n",
    "        xml_variant VARIANT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create a file format for the supplier transactions XML file\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT supplier_transactions_xml_ff \n",
    "    TYPE = 'XML' \n",
    "    STRIP_OUTER_ELEMENT=TRUE\n",
    "\"\"\")\n",
    "\n",
    "# Copy the supplier transactions XML file into the variant table\n",
    "cs.execute(\"\"\"\n",
    "    COPY INTO supplier_invoices_var \n",
    "    FROM @invoice_stg \n",
    "    FILE_FORMAT = (FORMAT_NAME = 'supplier_transactions_xml_ff') \n",
    "    ON_ERROR = 'CONTINUE' \n",
    "    FORCE = TRUE\n",
    "\"\"\")\n",
    "\n",
    "# Create supplier invoices table with data from the variant table\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE SUPPLIER_INVOICE AS \n",
    "    SELECT \n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<AmountExcludingTax>([^<]+)</AmountExcludingTax>', 1, 1, 'e') AS AmountExcludingTax,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<PurchaseOrderID>([^<]+)</PurchaseOrderID>', 1, 1, 'e') AS PurchaseOrderID,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<SupplierID>([^<]+)</SupplierID>', 1, 1, 'e') AS SupplierID,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<SupplierTransactionID>([^<]+)</SupplierTransactionID>', 1, 1, 'e') AS SupplierTransactionID,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<TransactionTypeID>([^<]+)</TransactionTypeID>', 1, 1, 'e') AS TransactionTypeID,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<PaymentMethodID>([^<]+)</PaymentMethodID>', 1, 1, 'e') AS PaymentMethodID,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<SupplierInvoiceNumber>([^<]+)</SupplierInvoiceNumber>', 1, 1, 'e') AS SupplierInvoiceNumber,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<TransactionDate>([^<]+)</TransactionDate>', 1, 1, 'e') AS TransactionDate,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<TaxAmount>([^<]+)</TaxAmount>', 1, 1, 'e') AS TaxAmount,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<TransactionAmount>([^<]+)</TransactionAmount>', 1, 1, 'e') AS TransactionAmount,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<OutstandingBalance>([^<]+)</OutstandingBalance>', 1, 1, 'e') AS OutstandingBalance,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<FinalizationDate>([^<]+)</FinalizationDate>', 1, 1, 'e') AS FinalizationDate,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<IsFinalized>([^<]+)</IsFinalized>', 1, 1, 'e') AS IsFinalized,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<LastEditedBy>([^<]+)</LastEditedBy>', 1, 1, 'e') AS LastEditedBy,\n",
    "        REGEXP_SUBSTR(xml_variant::STRING, '<LastEditedWhen>([^<]+)</LastEditedWhen>', 1, 1, 'e') AS LastEditedWhen\n",
    "    FROM supplier_invoices_var\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd43420d5d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "\n",
    "# Create or replace a view named \"PurchasesAndInvoices\" that joins the \"PURCHASES\" and \"SUPPLIER_INVOICE\" tables\n",
    "# The view includes columns from both tables where the \"PurchaseOrderID\" matches\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW PurchasesAndInvoices AS (\n",
    "    SELECT A.PurchaseOrderID, A.SupplierID, A.OrderDate, B.AmountExcludingTax, A.POAmount, B.TaxAmount, B.TransactionAmount\n",
    "    FROM PURCHASES A\n",
    "    INNER JOIN SUPPLIER_INVOICE B\n",
    "    ON A.PurchaseOrderID = B.PurchaseOrderID\n",
    ");\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd6091a9a90>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create or replace a view named \"purchase_orders_and_invoices\" that joins the \"PURCHASES\" and \"SUPPLIER_INVOICE\" tables\n",
    "# The view includes columns from both tables where the \"PurchaseOrderID\" matches\n",
    "# Additionally, it adds 5 years to the \"OrderDate\" and calculates the difference between \"AmountExcludingTax\" and \"POAmount\"\n",
    "\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW purchase_orders_and_invoices AS (\n",
    "    SELECT A.PurchaseOrderID, A.SupplierID, dateadd(year, +5, A.OrderDate) AS OrderDate, B.AmountExcludingTax - A.POAmount AS invoiced_vs_quoted, B.TaxAmount, B.TransactionAmount\n",
    "    FROM PURCHASES A\n",
    "    INNER JOIN SUPPLIER_INVOICE B\n",
    "    ON A.PurchaseOrderID = B.PurchaseOrderID\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries: csv for handling CSV files and psycopg2 for PostgreSQL database operations\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "# Define a function to export data from a PostgreSQL table to a CSV file\n",
    "def export_from_postgres_to_csv():\n",
    "    # Establish a connection to the PostgreSQL database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='WestCoastImporters',\n",
    "        user='jovyan',\n",
    "        password='postgres',\n",
    "        host='127.0.0.1',\n",
    "        port=8765\n",
    "    )\n",
    "    # Create a cursor object to interact with the database\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Open a new CSV file and write the data from the PostgreSQL table into it\n",
    "    with open('supplier_case.csv', 'w', newline='') as f:\n",
    "        cursor.copy_expert(\"COPY supplier_case TO STDOUT WITH CSV HEADER;\", f)\n",
    "    \n",
    "    # Close the cursor and the database connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Define a function to generate field definitions based on the first row of a CSV file\n",
    "def generate_field_definitions(csv_file_path):\n",
    "    # Open the CSV file in read mode\n",
    "    with open(csv_file_path, mode ='r') as file:\n",
    "        # Create a DictReader object to read the CSV file\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        \n",
    "        # Get the field names (column names) from the CSV file\n",
    "        field_names = csv_reader.fieldnames\n",
    "        \n",
    "        # Read the first row to determine the data types of the fields\n",
    "        first_row = next(csv_reader)\n",
    "        \n",
    "        # Initialize an empty list to hold field definitions\n",
    "        field_definitions = []\n",
    "        \n",
    "        # Loop through each field to determine its data type based on the first row\n",
    "        for field in field_names:\n",
    "            if first_row[field].isdigit():\n",
    "                field_type = 'INTEGER'\n",
    "            else:\n",
    "                field_type = 'STRING'\n",
    "            \n",
    "            # Append the field definition to the list\n",
    "            field_definitions.append(f\"{field} {field_type}\")\n",
    "        \n",
    "        # Return the field definitions as a comma-separated string\n",
    "        return ', '.join(field_definitions)\n",
    "\n",
    "\n",
    "def upload_to_snowflake(): \n",
    "    # Create internal stage\n",
    "    cs.execute(\"CREATE OR REPLACE STAGE supplier_stg FILE_FORMAT = (TYPE = 'CSV');\")\n",
    "    \n",
    "    # Upload CSV to internal stage\n",
    "    cs.execute(f\"PUT file://./supplier_case.csv @supplier_stg;\")\n",
    "    \n",
    "    # Generate CREATE TABLE statement\n",
    "    field_definitions = generate_field_definitions(\"supplier_case.csv\")\n",
    "    create_table_query = f\"CREATE TABLE supplier_case ({field_definitions});\"\n",
    "    \n",
    "    # Create the table\n",
    "    cs.execute(create_table_query)\n",
    "    \n",
    "    # Copy data from internal stage to the table\n",
    "    cs.execute(\"COPY INTO supplier_case FROM @supplier_stg FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER = 1) ON_ERROR = 'CONTINUE'\")\n",
    "    \n",
    "    cs.close()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_from_postgres_to_csv()\n",
    "    upload_to_snowflake()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd3a8170c50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "\n",
    "\n",
    "# Create or replace a table named \"noaa_gsod\" with specific columns and data types\n",
    "# The table is clustered by \"station_id\" and \"date\" for performance optimization\n",
    "# The data is sourced from another table \"ENVIRONMENT_DATA_ATLAS.ENVIRONMENT.NOAACD2019R\" and filtered and pivoted accordingly\n",
    "cs.execute(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TABLE noaa_gsod\n",
    "    CLUSTER BY (station_id, date) AS (\n",
    "    select *\n",
    "    from (\n",
    "        select \"Stations\", \"Date\", \"Stations Name\", \"Stations Latitude\", \"Stations Longitude\", \"Country\", \"Indicator Name\", \"Value\"\n",
    "        from ENVIRONMENT_DATA_ATLAS.ENVIRONMENT.NOAACD2019R\n",
    "        where \"Date\">='2020-01-01'\n",
    "        and \"Measure\"='M1'\n",
    "        and \"Country\"='US'\n",
    "    )\n",
    "    pivot(max(\"Value\") for \"Indicator Name\" in ('Mean visibility (miles)','Maximum temperature (Fahrenheit)','Mean dew point (Fahrenheit)','Maximum wind gust (Number)','Minimum temperature (Fahrenheit)','Maximum sustained wind speed (knots)','Mean wind speed (knots)','Mean station pressure (millibars)','Precipitation amount (inches)','Mean temperature (Fahrenheit)','Mean sea level pressure (millibars)','Snow depth (inches)')\n",
    "    ) as p(station_id, date, name, latitude, longitude, country_fips, visibility, max, dew, wind_max, min, wind_sustained_max, wind_mean, pressure, rain, temp, pressure_sea, snow_depth)\n",
    "    );\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd3a8170c50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "# Create or replace a stage named \"latlon_stg\" with CSV file format\n",
    "cs.execute(\"CREATE OR REPLACE STAGE latlon_stg FILE_FORMAT = (TYPE = 'CSV')\")\n",
    "\n",
    "# Define the Snowflake stage to upload files to\n",
    "stage = 'PROJECT_TEAM2.WORK.latlon_stg'\n",
    "\n",
    "\n",
    "# Function to guess the data type of a value\n",
    "def guess_data_type(value):\n",
    "    try:\n",
    "        int(value)\n",
    "        return 'INT'\n",
    "    except ValueError:\n",
    "        try:\n",
    "            float(value)\n",
    "            return 'FLOAT'\n",
    "        except ValueError:\n",
    "            return 'STRING'\n",
    "\n",
    "# Function to generate SQL for creating a table based on the first row of a CSV file\n",
    "def generate_create_table_sql(csv_path, table_name):\n",
    "    with open(csv_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)\n",
    "        first_row = next(reader)\n",
    "\n",
    "    column_types = [guess_data_type(value) for value in first_row]\n",
    "    column_definition = \", \".join(f\"{header} {data_type}\" for header, data_type in zip(headers, column_types))\n",
    "\n",
    "    create_table_sql = cs.execute(f\"CREATE OR REPLACE TABLE {table_name} ({column_definition})\")\n",
    "    return create_table_sql\n",
    "\n",
    "\n",
    "# Define the path to the CSV file and the table name\n",
    "csv_path = './simplemaps_uszips_basicv1.82/uszips.csv'\n",
    "table_name = 'LATLONGINFO'\n",
    "\n",
    "# Generate the SQL for creating the table and execute it\n",
    "create_table_sql = generate_create_table_sql(csv_path, table_name)\n",
    "\n",
    "# Upload the CSV file to the Snowflake stage\n",
    "cs.execute(f\"PUT file://./simplemaps_uszips_basicv1.82/uszips.csv @{stage}\")\n",
    "\n",
    "# Copy data from stage to table\n",
    "cs.execute(f\"COPY INTO LATLONGINFO FROM @{stage} FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER = 1) ON_ERROR = 'CONTINUE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd3a8170c50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.execute(\"USE PROJECT_TEAM2.WORK;\")\n",
    "\n",
    "# Create or replace a view named \"supplier_zip_code_weather\"\n",
    "# The view joins the \"SUPPLIER_CASE\", \"LATLONGINFO\", and \"NOAA_GSOD\" tables\n",
    "# It calculates the closest weather station to each supplier's ZIP code and selects the maximum temperature\n",
    "# The view is based on a CTE that ranks the weather stations by distance for each ZIP code\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW supplier_zip_code_weather AS (\n",
    "    WITH ranked_data AS (\n",
    "        SELECT A.POSTALPOSTALCODE, C.DATE, C.MAX, \n",
    "               st_distance(st_makepoint(B.LNG, B.LAT), st_makepoint(C.LONGITUDE, C.LATITUDE)) AS distance,\n",
    "               row_number() OVER (PARTITION BY A.POSTALPOSTALCODE ORDER BY st_distance(st_makepoint(B.LNG, B.LAT), st_makepoint(C.LONGITUDE, C.LATITUDE))) as rank\n",
    "        FROM SUPPLIER_CASE A\n",
    "        INNER JOIN LATLONGINFO B ON A.POSTALPOSTALCODE = B.ZIP\n",
    "        INNER JOIN NOAA_GSOD C ON st_distance(st_makepoint(B.LNG, B.LAT), st_makepoint(C.LONGITUDE, C.LATITUDE)) < 50000\n",
    "    )\n",
    "    SELECT POSTALPOSTALCODE, DATE, MAX(MAX) AS MAX_TEMP\n",
    "    FROM ranked_data\n",
    "    WHERE rank = 1\n",
    "    GROUP BY POSTALPOSTALCODE, DATE\n",
    "    ORDER BY POSTALPOSTALCODE, DATE\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fd6094eab90>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create or replace a view named \"combined_orders_and_weather\"\n",
    "# The view joins the \"purchase_orders_and_invoices\", \"supplier_zip_code_weather\", and \"supplier_case\" tables\n",
    "# It selects various columns from these tables, including supplier ID, supplier name, purchase order ID, order date, and maximum temperature\n",
    "# The view filters out records where the maximum temperature is null\n",
    "\n",
    "\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW combined_orders_and_weather AS (\n",
    "    SELECT C.supplierid, C.suppliername, A.purchaseorderid, A.orderdate, B.postalpostalcode, B.MAX_TEMP\n",
    "    FROM purchase_orders_and_invoices A\n",
    "    INNER JOIN supplier_zip_code_weather B ON A.Orderdate = B.Date\n",
    "    INNER JOIN supplier_case C ON B.POSTALPOSTALCODE = C.POSTALPOSTALCODE\n",
    "    WHERE B.MAX_TEMP IS NOT NULL\n",
    ");\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
